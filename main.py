import asyncio
import yaml
import os
import sys
import argparse
from pathlib import Path
PROJECT_ROOT = Path(__file__).resolve().parent
if str(PROJECT_ROOT) not in sys.path:
	sys.path.insert(0, str(PROJECT_ROOT))

from src.core.engine import CrawlerEngine
from src.scrapers.icml import ICMLScraper
from src.scrapers.neurips import NeurIPSScraper
from src.scrapers.iclr import ICLRScraper
from src.scrapers.openalex import OpenAlexScraper


def load_config(path="config.yaml"):
	if not os.path.exists(path):
		raise FileNotFoundError(f"Config file not found at {path}")
	with open(path, "r", encoding="utf-8") as f:
		return yaml.safe_load(f)


def parse_args():
	parser = argparse.ArgumentParser(description="Paper-Tunneling CLI")
	parser.add_argument("--keywords", nargs="+", help="å…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚: quantum qaoa")
	parser.add_argument("--years", nargs="+", type=int, help="å¹´ä»½åˆ—è¡¨ï¼Œä¾‹å¦‚: 2023 2024 2025")
	parser.add_argument("--conferences", nargs="+", help="ä¼šè®®åˆ—è¡¨ï¼Œä¾‹å¦‚: icml")
	parser.add_argument("--journals", nargs="+", help="æœŸåˆŠåˆ—è¡¨ï¼Œä¾‹å¦‚: nmi")
	parser.add_argument("--concurrency", type=int, help="å¹¶å‘è¯·æ±‚æ•°ï¼Œä¾‹å¦‚: 5")
	return parser.parse_args()


def _sanitize_token(token: str) -> str:
	allowed = set("abcdefghijklmnopqrstuvwxyz0123456789-_+")
	return "".join(ch for ch in token.lower() if ch in allowed)



def build_output_filename(config):
	confs = config.get("conferences") or []
	journals = config.get("journals") or []
	if not confs and journals:
		confs = journals
	if not confs:
		confs = ["papers"]
	keywords = config.get("keywords") or ["all"]
	years = config.get("years") or ["all"]

	conf_part = "_".join(_sanitize_token(c) for c in confs)
	keyword_part = "_".join(_sanitize_token(k) for k in keywords)
	year_part = "_".join(str(y) for y in years)
	return f"{conf_part}_{keyword_part}_{year_part}.md"


async def main():
	print("ğŸš€ Starting Paper-Tunneling...")
    
	# 1. åŠ è½½é…ç½®
	config = load_config()
	args = parse_args()

	# 1.1 CLI è¦†ç›–é…ç½®
	cli_override = False
	if args.keywords:
		config["keywords"] = args.keywords
		cli_override = True
	if args.years:
		config["years"] = args.years
		cli_override = True
	if args.conferences:
		config["conferences"] = [c.lower() for c in args.conferences]
		cli_override = True
	if args.journals:
		config["journals"] = [j.lower() for j in args.journals]
		cli_override = True
	if args.concurrency is not None:
		config["concurrency"] = args.concurrency
		cli_override = True

	if cli_override:
		config["output_filename"] = build_output_filename(config)

	conferences = config.get("conferences")
	journals = config.get("journals") or []

	# ä»…æŒ‡å®šæœŸåˆŠæ—¶ï¼Œä¸é»˜è®¤å¯ç”¨ä¼šè®®
	if conferences is None and journals:
		conferences = []
	if conferences is None:
		conferences = ["icml"]
	def _slug_name(text: str) -> str:
		return "-".join(text.lower().split())

	journal_aliases = {
		"nmi": "nature-machine-intelligence",
		"ncs": "nature-computational-science",
		"npjqi": "npj-quantum-information",
		"prl": "physical-review-letters",
		"quantum": "quantum",
	}

	conferences = [c.lower() for c in conferences]
	journals = [journal_aliases.get(j, j).lower() for j in journals]
	config["conferences"] = conferences
	config["journals"] = journals

	targets = config.get("targets", [])
	selected_targets = []
	if journals:
		journal_set = set(journals)
		for target in targets:
			name = target.get("name", "")
			issn = (target.get("issn", "") or "").lower()
			slug = _slug_name(name)
			if issn in journal_set or slug in journal_set:
				selected_targets.append(target)
				continue
			alias = journal_aliases.get(journals[0], "") if journals else ""
			if alias and slug == alias:
				selected_targets.append(target)

	if selected_targets:
		config["journals"] = [_slug_name(t.get("name", "")) for t in selected_targets]
    
	# 2. åˆå§‹åŒ–çˆ¬è™«åˆ—è¡¨ (ç›®å‰åªæœ‰ ICML)
	scrapers = []
    
	if "icml" in conferences:
		icml_scraper = ICMLScraper(config)
		scrapers.append(icml_scraper)
	if "neurips" in conferences:
		neurips_scraper = NeurIPSScraper(config)
		scrapers.append(neurips_scraper)
	if "iclr" in conferences:
		iclr_scraper = ICLRScraper(config)
		scrapers.append(iclr_scraper)
	for target in selected_targets:
		scrapers.append(OpenAlexScraper(config, target))

	if not scrapers:
		raise ValueError("No valid conferences selected. Currently supported: icml, neurips, iclr, openalex targets")
    
	# 3. å¯åŠ¨å¼•æ“
	engine = CrawlerEngine(scrapers, config)
	await engine.run()
    
	print("\nâœ… Job Done! Check the 'results' folder.")


if __name__ == "__main__":
	# Windows å…¼å®¹æ€§å¤„ç†
	if os.name == 'nt':
		asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
	asyncio.run(main())
